# ai-counsel/config.yaml
version: "1.0"

cli_tools:
  claude:
    command: "claude"
    args:
      [
        "-p",
        "--model",
        "{model}",
        "--settings",
        '{{"disableAllHooks": true}}',
        "{prompt}",
      ]
    timeout: 300
    # Valid models: "sonnet", "opus", "haiku" or full names like "claude-sonnet-4-5-20250929"
    # Note: -p flag is auto-removed during deliberations for full engagement

  codex:
    command: "codex"
    args: ["exec", "--model", "{model}", "{prompt}"]
    timeout: 180
    # Valid models: "gpt-5-codex", "o3", or other OpenAI model identifiers

  droid:
    command: "droid"
    args: ["exec", "-m", "{model}", "{prompt}"]
    timeout: 180
    # ONLY VALID MODELS: "claude-sonnet-4-5-20250929", "gpt-5-codex", "Droid Core (GLM-4.6)"
    # Note: Requires FULL model IDs, not short aliases
    #
    # Adaptive Permission Strategy (Graceful Degradation):
    # The Droid adapter automatically handles permission levels without manual configuration.
    # If a permission error occurs (e.g., "insufficient permission to proceed"), the adapter
    # will automatically retry with higher permission levels:
    #   1. First attempt: --auto low (basic file reading, safe for text generation)
    #   2. If fails: --auto medium (allows network/git operations for context)
    #   3. If fails: --auto high (allows production operations if needed)
    # This ensures deliberations work seamlessly regardless of Droid's permission requirements.

  gemini:
    command: "gemini"
    args: ["-m", "{model}", "-p", "{prompt}"]
    timeout: 180
    # Valid models: "gemini-2.5-pro" (default) or other Gemini identifiers

  llamacpp:
    command: "llama-cli"
    args: ["-m", "{model}", "-p", "{prompt}", "-n", "512", "-c", "2048"]
    timeout: 180
    # llama.cpp is a fast, lightweight LLM inference engine for running models locally
    # Valid models: path to .gguf model file (e.g., "/path/to/llama-2-7b.Q4_K_M.gguf")
    # Common args:
    #   -m: model path (REQUIRED - use {model} placeholder)
    #   -p: prompt text (REQUIRED - use {prompt} placeholder)
    #   -n: number of tokens to predict (default: 128, recommended: 512+)
    #   -c: context size (default: 512, recommended: 2048-4096)
    #   -t: threads (default: auto, e.g., "-t", "8")
    #   --temp: temperature (e.g., "--temp", "0.7")
    # Download models from https://huggingface.co/models?library=gguf
    # Build llama.cpp: https://github.com/ggerganov/llama.cpp

# HTTP Adapters Section (new format)
adapters:
  ollama:
    type: http
    base_url: "http://localhost:11434"
    timeout: 120
    max_retries: 3
    # Valid models: llama2, mistral, codellama, qwen, etc.
    # Run 'ollama list' to see available models
    # Ollama is a local LLM runtime - no API key needed
  #
  lmstudio:
    type: http
    base_url: "http://localhost:1234"
    timeout: 120
    max_retries: 3
    # Valid models: any model loaded in LM Studio
    # LM Studio provides OpenAI-compatible API
    # No API key needed for local instance

  openrouter:
    type: http
    base_url: "https://openrouter.ai/api/v1"
    api_key: "${OPENROUTER_API_KEY}" # Environment variable from .env file
    timeout: 90
    max_retries: 3
    # Valid models: anthropic/claude-3.5-sonnet, openai/gpt-4, meta-llama/llama-3.1-8b-instruct, etc.
    # See https://openrouter.ai/docs for full model list
    # Requires API key from https://openrouter.ai/keys

defaults:
  mode: "quick"
  rounds: 2
  max_rounds: 5
  timeout_per_round: 120

model_registry:
  claude:
    - id: "claude-sonnet-4-5-20250929"
      label: "Claude Sonnet 4.5"
      tier: "balanced"
      default: true
    - id: "claude-haiku-4-5-20251001"
      label: "Claude Haiku 4.5"
      tier: "speed"
    - id: "claude-opus-4-1-20250805"
      label: "Claude Opus 4.1"
      tier: "premium"
  codex:
    - id: "gpt-5-codex"
      label: "GPT-5 Codex"
      tier: "coding"
      default: true
    - id: "gpt-5"
      label: "GPT-5"
      tier: "general"
  droid:
    - id: "claude-sonnet-4-5-20250929"
      label: "Claude Sonnet 4.5 (via Droid)"
      tier: "balanced"
      default: true
    - id: "gpt-5-codex"
      label: "GPT-5 Codex (via Droid)"
      tier: "coding"
    - id: "glm-4.6"
      label: "Droid Core (GLM-4.6)"
      tier: "open-source"
  gemini:
    - id: "gemini-2.5-pro"
      label: "Gemini 2.5 Pro"
      tier: "general"
      default: true

storage:
  transcripts_dir: "transcripts"
  format: "markdown"
  auto_export: true

mcp:
  # Maximum rounds to include in MCP response (to avoid token limit)
  # Full transcript is always saved to file - this only affects MCP response size
  max_rounds_in_response: 3

deliberation:
  # Convergence detection settings
  convergence_detection:
    enabled: true

    # Similarity thresholds
    semantic_similarity_threshold: 0.85 # Models converged if similarity >= this
    divergence_threshold: 0.40 # Models diverging if similarity < this

    # Round constraints
    min_rounds_before_check: 1 # Check convergence starting from round 2 (need 2 rounds to compare)
    consecutive_stable_rounds: 2 # Require 2 stable rounds to confirm

    # Secondary metrics
    stance_stability_threshold: 0.80 # 80% of participants must have stable stances
    response_length_drop_threshold: 0.40 # Flag if response length drops >40%

  # Model-controlled early stopping
  early_stopping:
    enabled: true
    threshold: 0.66 # Stop if >=66% of models want to stop (2/3 consensus)
    respect_min_rounds: true # Don't stop before defaults.rounds is reached

  # Legacy settings (keep these)
  convergence_threshold: 0.8
  enable_convergence_detection: true

# Decision Graph Memory
decision_graph:
  enabled: true # Feature toggle (opt-in)
  db_path: "decision_graph.db" # Relative to project root - works for any user

  # DEPRECATED: similarity_threshold is no longer used. Use tier_boundaries instead.
  similarity_threshold: 0.6 # Minimum similarity score for context injection (0.0-1.0)

  # NEW: Budget-aware context injection parameters
  context_token_budget: 1500 # Max tokens for context injection (prevents token bloat)
  tier_boundaries:
    strong: 0.75 # Strong matches get full formatting (~500 tokens each)
    moderate: 0.60 # Moderate matches get summary formatting (~200 tokens each)
  query_window: 1000 # Recent decisions to query (scalability limit)

  # Keep for backward compatibility
  max_context_decisions: 3 # Maximum number of past decisions to inject as context
  compute_similarities: true # Compute edge similarities after storing deliberation
